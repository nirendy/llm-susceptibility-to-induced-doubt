\maketitle
\begin{abstract}
This study investigates the impact of iterative prompting on Large Language Model (LLM) responses, focusing on how expressions of doubt influence the consistency and reliability of answers across various models of comparable size. Using a methodology that presents LLMs with binary-choice questions followed by expressions of doubt in varying settings, we analyze how different prompting strategies impact their behavior. By comparing the responses of
different LLM architectures under the tested
prompting strategies, we aim to uncover patterns in how these models react to various forms of iterative prompting. Our research contributes to the growing body of knowledge on LLM behavior, offering insights into the malleability of AI-generated content and the potential for guiding or manipulating LLM outputs.
\end{abstract}