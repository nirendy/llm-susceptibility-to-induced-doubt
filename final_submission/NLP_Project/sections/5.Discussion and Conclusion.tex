\section{Discussion and Conclusion}

This study investigated the effects of expressions of doubt, and iterative feedback on the performance of large language models (LLMs) of varying architectures and sizes. By analyzing their responses to binary-choice questions across multiple experiments, we uncovered key insights into how LLMs process uncertainty and adjust their outputs.

Our findings highlight the nuanced interplay between model size, feedback, and prompting strategies. Smaller models consistently showed a decline in accuracy when doubt was introduced, but when diving deeper into the results, and controlling for the posi
 suggesting susceptibility to uncertainty due to limited internal representations and weaker generalization. These models were prone to "correct-to-incorrect" transitions (V$\to$X) (Experiment 2), further emphasizing their fragility. Conversely, larger models demonstrated the ability to leverage doubt prompts to reassess their initial responses, improving accuracy through "incorrect-to-correct" transitions (X$\to$V). They also adjusted their confidence levels effectively, as seen in the logit analysis (Experiment 4), reflecting robust decision-making processes.

Prompt variations and answer positioning (Experiment 3) revealed critical areas for improvement. Positional bias significantly impacted smaller models, suggesting that they struggled to interpret prompts fully. While larger models were more robust to these variations, example-based prompts—designed for few-shot learning—had mixed effects. Surprisingly, certain configurations (e.g., example order) improved performance in larger models like Mixtral but confused smaller ones, illustrating the need for tailored prompting strategies.

Iterative feedback combined with doubt (Experiment 5) showed mixed results. While feedback enhanced accuracy in some cases, iterative questioning alone—without doubt—yielded similar improvements, especially in larger models. This suggests that introducing doubt may not always be necessary to stabilize and enhance performance, particularly during a "warm-up" phase. Stable models, such as Phi, exhibited consistent performance regardless of the iterative strategy, while others, like Nemo, showed unique behavior where accuracy improved post-doubt but degraded with feedback.

Taken together, these results emphasize the importance of model size in determining susceptibility to uncertainty and the utility of iterative prompting strategies. Larger models benefit from robust internal mechanisms that enable them to adapt effectively to feedback and doubt, while smaller models require targeted interventions to improve resilience.

\subsection{Limitations}
This research focused on binary-choice questions, which may not fully generalize to more complex tasks. Additionally, the scope of model architectures and sizes, as well as the limited exploration of feedback strategies, may constrain the broader applicability of our findings. Future studies should address these gaps to provide a more comprehensive understanding of LLM behavior.