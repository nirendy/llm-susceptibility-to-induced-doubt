\section{Discussion and Conclusion}

This study explored how expressions of doubt and iterative questioning influence the performance of Large Language Models (LLMs), shedding light on their robustness, adaptability, and decision-making processes. Our findings reveal critical insights into how model size, prompt design, and confidence metrics affect LLM behavior, with broader implications for the design and deployment of these systems.

\textbf{Role of Model Size} 
A clear relationship between model size and response to doubt emerged across experiments. Larger models (>8B parameters) demonstrated the ability to leverage doubt prompts as opportunities for reassessment, resulting in improved accuracy and stable confidence. In contrast, smaller models (3B and below) were destabilized by expressions of doubt, leading to a vast switch of response. This behavior suggests that larger models possess more robust internal representations and decision-making mechanisms, while smaller models remain vulnerable to uncertainty.

\textbf{Limitations in Prompt Comprehension} 
Positional bias in smaller models underscores challenges in prompt comprehension. Despite the assumption that prompt variations would not significantly impact performance, experiments revealed that even slight changes in prompt design could alter accuracy, particularly in smaller models. This highlights the need for refined prompt engineering to ensure consistency and reliability across varying contexts.

\textbf{Confidence as a Diagnostic Tool}
Confidence metrics provided valuable insights into the underlying decision-making processes of models. Larger models maintained higher confidence in correct answers, even after expressing doubt, while smaller models showed more significant fluctuations. The use of logit differences allowed us to identify questions where the models struggled, offering a granular understanding of their internal certainty.

\textbf{Iterative Questioning}
While induced doubt sometimes negatively impacted performance, iterative questioning without doubt proved effective in improving accuracy, particularly in larger models. This finding suggests a potential ``warm-up'' phase for models to stabilize their responses before more complex interactions, offering practical value for real-world applications.

% \subsection{Implications}

% These results emphasize the importance of scaling model size to enhance robustness and reliability. Additionally, the findings highlight the value of confidence-based evaluations as a complementary metric to accuracy, particularly for identifying areas of uncertainty and instability. Finally, optimizing prompt designs and exploring iterative questioning frameworks can contribute to the development of more adaptable and user-friendly LLMs.

% By understanding how LLMs respond to doubt, we can guide the design of systems that better handle uncertainty, improve decision-making, and ultimately provide more reliable AI-generated content.